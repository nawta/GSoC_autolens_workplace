{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. Using a deep learning algorithm of your choice, learn the representation of dark matter in the strong gravitational lensing images provided using PyTorch.\n",
    "The goal is to use this learned representation for anomaly detection, hence you should pick the most appropriate algorithm and discuss your strategy.   \n",
    "   \n",
    "A. Considering the purpose is anomaly detection, I choose Autoencoder as the most appropriate method, as discussed in [1]. I think Conditional VAE (CVAE)[5] especially suits this purpose.   \n",
    "Deep neural network architecture is known to function relatively better in tasks about images than MLP(multilayer perception), and in the early stage Convolutional AutoEncoder(hereafter CAE)[2] and Variational AutoEncoder(VAE)[8] was the most competent method.\n",
    "Of cource, it is said that CNN architecture in general performs well, speaking of image processing.  \n",
    "CAE incorpolates CNN's algorithm into itself, and according to [3],[4] (written in Japnanese), it clearly shows that CAE overwhelms other MLP algorithms.   \n",
    "VAE incorpolates probability distribution into latent variable z, and it also performs as well as CAE does.\n",
    "These days, lots of methods derived from these, and I noticed CVAE is the best, which add supervising features to VAE. it is compared with lots of other autoencoder methods on purpose of analysis on astronomical images[6], and prove that the latent variables obtained with CVAE preserve more information and are more discriminative towards real astronomical transients.(SCAE[7] did well as well, but slightly CVAE has an advantage of it)    \n",
    "This paper was published in April 2018, and it is the most recent paper applying autoencoder to processing astronomical images.(According to my survey on Google Scholor)\n",
    "In addition to that, CVAE is implementable on laptop, whereas other methods sometimes need a bulk of computer properties(such as [9].)\n",
    "\n",
    "Using such autoencoders, anomaly detection will be easy.(in future work I can impliment heat map which clearly tells anomalies from norms [10] )\n",
    "\n",
    "According to Hold-out method, I devided images into train(3001 images) and test(2000 images).  \n",
    "\n",
    "Of cource, CNN could be used because this problem can be taken as a classification problem(so no_sub images provided)\n",
    "\n",
    "About optimizer, I use Adam because this is one of the most popular optimizer.\n",
    "\n",
    "\n",
    "in inplementation, I refered to this site:  \n",
    "https://graviraja.github.io/conditionalvae/#\n",
    "\n",
    "----\n",
    "\n",
    "[1]Deep Learning the Morphology of Dark Matter Substructure, pp8\n",
    "https://arxiv.org/pdf/1909.07346.pdf\n",
    "\n",
    "[2]Deep Clustering with Convolutional Autoencoders\n",
    "https://link.springer.com/chapter/10.1007/978-3-319-70096-0_39\n",
    "\n",
    "[3]PytorchによるAutoEncoder Familyの実装(Implementation of various Autoencoders using PyTorch)\n",
    "https://elix-tech.github.io/ja/2016/07/17/autoencoder.html\n",
    "\n",
    "[4]様々なオートエンコーダによる異常検知(Anomaly Detection with lots of Autoencoders)\n",
    "https://sinyblog.com/deaplearning/auto_encoder_001/\n",
    "\n",
    "[5]Semi-Supervised Learning with Deep Generative Models (CVAE)\n",
    "https://arxiv.org/abs/1406.5298\n",
    "\n",
    "[6]Latent representations of transient candidates from an astronomical image difference pipeline using Variational Autoencoders\n",
    "https://pdfs.semanticscholar.org/57ce/a9520008706b8cb190d94513e695068210cd.pdf\n",
    "\n",
    "[7]Convolutional Sparse Autoencoders for Image Classification (SCAE)\n",
    "https://ieeexplore.ieee.org/document/7962256\n",
    "\n",
    "[8]Auto-Encoding Variational Bayes(VAE)\n",
    "https://arxiv.org/pdf/1312.6114.pdf\n",
    "\n",
    "[9]Improving Variational Autoencoder with Deep Feature Consistent and Generative Adversarial Training (GAN)\n",
    "https://arxiv.org/pdf/1906.01984.pdf\n",
    "\n",
    "[10]Anomaly Manufacturing Product Detection using Unregularized Anomaly Score on Deep Generative Models\n",
    "https://confit.atlas.jp/guide/event-img/jsai2018/2A1-03/public/pdf?type=in\n",
    "https://qiita.com/shinmura0/items/811d01384e20bfd1e035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms, utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128        # number of data points in each batch\n",
    "N_EPOCHS = 10           # times to run the model on complete data\n",
    "INPUT_DIM = 150 * 150     # size of each input\n",
    "HIDDEN_DIM = 256        # hidden dimension\n",
    "LATENT_DIM = 50         # latent vector dimension\n",
    "lr = 1e-3               # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([transforms.RandomSizedCrop(INPUT_DIM), transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(root='./lenses/train', #3001 images each\n",
    "                                           transform=data_transform)\n",
    "test_dataset = datasets.ImageFolder(root='./lenses/test', #2000 images each\n",
    "                                           transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx2onehot(idx, n=N_CLASSES):\n",
    "\n",
    "    assert idx.shape[1] == 1\n",
    "    assert torch.max(idx).item() < n\n",
    "\n",
    "    onehot = torch.zeros(idx.size(0), n)\n",
    "    onehot.scatter_(1, idx.data, 1)\n",
    "\n",
    "    return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, z_dim):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim: A integer indicating the size of input (in case of MNIST 28 * 28).\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            z_dim: A integer indicating the latent dimension.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.mu = nn.Linear(hidden_dim, z_dim)\n",
    "        self.var = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "\n",
    "        hidden = F.relu(self.linear(x))\n",
    "        # hidden is of shape [batch_size, hidden_dim]\n",
    "        z_mu = self.mu(hidden)\n",
    "        # z_mu is of shape [batch_size, latent_dim]\n",
    "        z_var = self.var(hidden)\n",
    "        # z_var is of shape [batch_size, latent_dim]\n",
    "\n",
    "        return z_mu, z_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "encoder = Encoder(INPUT_DIM, HIDDEN_DIM, LATENT_DIM)\n",
    "\n",
    "# decoder\n",
    "decoder = Decoder(LATENT_DIM, HIDDEN_DIM, INPUT_DIM)\n",
    "\n",
    "# vae\n",
    "model = VAE(encoder, decoder).to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # set the train mode\n",
    "    model.train()\n",
    "\n",
    "    # loss of the epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, (x, _) in enumerate(train_iterator):\n",
    "        # reshape the data into [batch_size, 784]\n",
    "        x = x.view(BATCH_SIZE*3, 150 * 150)\n",
    "        x = x.to(device)\n",
    "\n",
    "        # update the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        x_sample, z_mu, z_var = model(x)\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss = F.binary_cross_entropy(x_sample, x, size_average=False)\n",
    "\n",
    "        # kl divergence loss\n",
    "        kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1.0 - z_var)\n",
    "\n",
    "        # total loss\n",
    "        loss = recon_loss + kl_loss\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test():\n",
    "    # set the evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # test loss for the data\n",
    "    test_loss = 0\n",
    "\n",
    "    # we don't need to track the gradients, since we are not updating the parameters during evaluation / testing\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(test_iterator):\n",
    "            # reshape the data\n",
    "            x = x.view(BATCH_SIZE*3, 150 * 150)\n",
    "            x = x.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            x_sample, z_mu, z_var = model(x)\n",
    "\n",
    "            # reconstruction loss\n",
    "            recon_loss = F.binary_cross_entropy(x_sample, x, size_average=False)\n",
    "\n",
    "            # kl divergence loss\n",
    "            kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1.0 - z_var)\n",
    "\n",
    "            # total loss\n",
    "            loss = recon_loss + kl_loss\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_loss = float('inf')\n",
    "\n",
    "for e in range(N_EPOCHS):\n",
    "\n",
    "    train_loss = train()\n",
    "    test_loss = test()\n",
    "\n",
    "    train_loss /= len(train_dataset)\n",
    "    test_loss /= len(test_dataset)\n",
    "\n",
    "    print(f'Epoch {e}, Train Loss: {train_loss:.2f}, Test Loss: {test_loss:.2f}')\n",
    "\n",
    "    if best_test_loss > test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        patience_counter = 1\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample and generate a image\n",
    "z = torch.randn(1, LATENT_DIM).to(device)\n",
    "reconstructed_img = model.dec(z)\n",
    "img = reconstructed_img.view(150, 150).data\n",
    "\n",
    "print(z.shape)\n",
    "print(img.shape)\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
